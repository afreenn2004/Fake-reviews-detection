# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nnd4m_jFcXzAOlqtGb5vaM-W5BwL77mu

import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
from sklearn.exceptions import NotFittedError

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

# Load metadata model and scaler
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    return metadata_model, scaler

# Extract metadata features
def extract_metadata_features(text):
    tokens = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    num_words = len(tokens)
    num_sentences = len(nltk.sent_tokenize(text))
    num_nouns = len([word for word, pos in pos_tags if pos.startswith('NN')])
    num_adjectives = len([word for word, pos in pos_tags if pos.startswith('JJ')])
    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0
    features = {
        'num_words': num_words,
        'num_sentences': num_sentences,
        'num_nouns': num_nouns,
        'num_adjectives': num_adjectives,
        'avg_word_length': avg_word_length
    }
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, inputs

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

# Explain metadata model
def explain_with_metadata(text, metadata_model, scaler):
    features = extract_metadata_features(text)
    feature_values = np.array([list(features.values())])
    try:
        scaled_features = scaler.transform(feature_values)
    except NotFittedError:
        raise ValueError("‚ö†Ô∏è The StandardScaler is not fitted. Please fit it during training before saving.")
    probs = metadata_model.predict_proba(scaled_features)
    return probs, features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler = load_metadata_model()

    user_review = st.text_area("Enter your review:", "")

    if st.button("Predict"):
        if not user_review.strip():
            st.warning("Please enter a review.")
        else:
            with st.spinner("Making prediction and generating explanations..."):
                # BERT prediction
                pred, confidence, inputs = predict_bert(user_review, model, tokenizer)
                label = "üü¢ Genuine Review (OR)" if pred == 0 else "üî¥ Fake Review (CG)"
                st.subheader(f"Prediction: {label}")
                st.write(f"**Confidence:** {confidence:.2f}")

                # SHAP explanation for BERT
                st.subheader("BERT Model Explanation (SHAP)")
                try:
                    st.text("Words that most influenced the prediction:")
                    shap_html = shap.plots.text(shap_values[0], display=False)  # capture HTML
                    components.html(shap_html, height=300, scrolling=True)
                except Exception as e:
                    st.error(f"Error generating SHAP explanation: {e}")

                # Metadata explanation
                st.subheader("Metadata Model Explanation")
                try:
                    shap_values_meta, features = explain_with_metadata(user_review, metadata_model, scaler)
                    st.write("**Extracted Metadata Features:**")
                    st.json(features)
                    st.write("**Metadata Model Prediction Probabilities:**")
                    st.write(f"üî¥ Fake: {shap_values_meta[0][0]:.2f} &nbsp;&nbsp; üü¢ Genuine: {shap_values_meta[0][1]:.2f}")
                except Exception as e:
                    st.error(f"Error generating metadata explanation: {e}")

if __name__ == "__main__":
    main()
    
import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
from sklearn.exceptions import NotFittedError
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

# Load metadata model and scaler
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    feature_names = [
        'num_words',
        'num_sentences', 
        'num_nouns',
        'num_adjectives',
        'avg_word_length'
    ]
    return metadata_model, scaler, feature_names

# Extract metadata features
def extract_metadata_features(text):
    tokens = nltk.word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    num_words = len(tokens)
    num_sentences = len(nltk.sent_tokenize(text))
    num_nouns = len([word for word, pos in pos_tags if pos.startswith('NN')])
    num_adjectives = len([word for word, pos in pos_tags if pos.startswith('JJ')])
    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0
    
    features = {
        'num_words': num_words,
        'num_sentences': num_sentences,
        'num_nouns': num_nouns,
        'num_adjectives': num_adjectives,
        'avg_word_length': avg_word_length
    }
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs
    
    explainer = shap.Explainer(f, tokenizer)
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, feature_names, features_dict):
    # Prepare input data
    feature_values = np.array([[features_dict[col] for col in feature_names]])
    scaled_features = scaler.transform(feature_values)
    
    # Create SHAP explainer if not cached
    if 'metadata_explainer' not in st.session_state:
        st.session_state.metadata_explainer = shap.KernelExplainer(
            metadata_model.predict_proba,
            shap.sample(scaler.transform(np.zeros((1, len(feature_names))), 
            link="logit"
        )))
    
    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value
    
    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, feature_names = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
        else:
            with st.spinner("Analyzing review and generating explanations..."):
                # Make predictions
                bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
                label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"
                
                # Display prediction
                st.subheader(f"Prediction: {label}")
                st.write(f"**Confidence:** {confidence:.2f}")
                st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

                # BERT SHAP Explanation
                st.subheader("BERT Model Explanation")
                try:
                    shap_values = explain_with_shap(user_review, model, tokenizer)
                    
                    # Text plot
                    st.write("**Word Importance:**")
                    shap_text_plot = shap.plots.text(shap_values[0], display=False)
                    components.html(shap_text_plot, height=300, scrolling=True)
                    
                    # Waterfall plot
                    st.write("**Feature Impact:**")
                    plt.figure()
                    shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)
                except Exception as e:
                    st.error(f"Error generating BERT SHAP explanation: {str(e)}")

                # Metadata Analysis
                st.subheader("Metadata Analysis")
                try:
                    features = extract_metadata_features(user_review)
                    st.write("**Extracted Features:**")
                    st.json(features)
                    
                    # Metadata SHAP Explanation
                    shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap(
                        metadata_model, scaler, feature_names, features
                    )
                    
                    # Handle binary vs multi-class
                    if isinstance(shap_values_meta, list):
                        shap_values_meta = shap_values_meta[bert_pred][0]
                        expected_value = expected_value[bert_pred]
                    else:
                        shap_values_meta = shap_values_meta[0, :, bert_pred]
                        expected_value = expected_value[bert_pred]
                    
                    # Create Explanation object
                    explanation = shap.Explanation(
                        values=shap_values_meta,
                        base_values=expected_value,
                        data=scaled_features[0],
                        feature_names=feature_names
                    )
                    
                    # Bar plot
                    st.write("**Feature Importance:**")
                    plt.figure()
                    shap.plots.bar(explanation, max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)
                    
                    # Force plot
                    st.write("**Impact on Prediction:**")
                    plt.figure()
                    shap.plots.force(
                        expected_value,
                        shap_values_meta,
                        features=scaled_features[0],
                        feature_names=feature_names,
                        matplotlib=True,
                        show=False
                    )
                    st.pyplot(plt.gcf(), clear_figure=True)
                    
                except Exception as e:
                    st.error(f"Error generating metadata explanation: {str(e)}")

if __name__ == "__main__":
    main()
    

import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
from sklearn.exceptions import NotFittedError
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords') # Add stopwords download

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

# Load metadata model and scaler
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = CountVectorizer(max_features=100, stop_words='english')
    feature_names = vectorizer.get_feature_names_out().tolist() # Get word features

    feature_names = [
        'sentiment',  # Changed order to match notebook
        *feature_names, # Add word features
        'review_length',
        'uppercase_word_count',
        'exclamation_count',
        'question_count',
        'punctuation_count',
        'avg_word_length',
        'flesch_reading_ease',
        'stopword_count'
    ]
    return metadata_model, scaler, feature_names, vectorizer # Return vectorizer

# Extract metadata features (Modified to match notebook)
def extract_metadata_features(text):
    features = {}

    if isinstance(text, str):
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stopwords.words('english'))
        features['sentiment'] = SentimentIntensityAnalyzer().polarity_scores(text)['compound']
    else:
        features['review_length'] = 0
        features['uppercase_word_count'] = 0
        features['exclamation_count'] = 0
        features['question_count'] = 0
        features['punctuation_count'] = 0
        features['avg_word_length'] = 0
        features['flesch_reading_ease'] = 0
        features['stopword_count'] = 0
        features['sentiment'] = 0

    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, tokenizer)
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, feature_names, features_dict):
    # Prepare input data
    feature_values = np.array([[features_dict[col] for col in feature_names]])
    scaled_features = scaler.transform(feature_values)

    # Create SHAP explainer if not cached
    if 'metadata_explainer' not in st.session_state:
        st.session_state.metadata_explainer = shap.KernelExplainer(
            metadata_model.predict_proba,
            shap.sample(scaler.transform(np.zeros((1, len(feature_names))), 
                        link="logit"
            )))

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, feature_names = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
        else:
            with st.spinner("Analyzing review and generating explanations..."):
                # Make predictions
                bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
                label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"

                # Display prediction
                st.subheader(f"Prediction: {label}")
                st.write(f"**Confidence:** {confidence:.2f}")
                st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

                # BERT SHAP Explanation
                st.subheader("BERT Model Explanation")
                try:
                    shap_values = explain_with_shap(user_review, model, tokenizer)

                    # Text plot
                    st.write("**Word Importance:**")
                    shap_text_plot = shap.plots.text(shap_values[0], display=False)
                    components.html(shap_text_plot, height=300, scrolling=True)

                    # Waterfall plot
                    st.write("**Feature Impact:**")
                    plt.figure()
                    shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)
                except Exception as e:
                    st.error(f"Error generating BERT SHAP explanation: {str(e)}")

                # Metadata Analysis
                st.subheader("Metadata Analysis")
                try:
                    features = extract_metadata_features(user_review)
                    st.write("**Extracted Features:**")
                    st.json(features)

                    # Metadata SHAP Explanation
                    shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap(
                        metadata_model, scaler, feature_names, features
                    )

                    # Handle binary vs multi-class
                    if isinstance(shap_values_meta, list):
                        shap_values_meta = shap_values_meta[bert_pred][0]
                        expected_value = expected_value[bert_pred]
                    else:
                        shap_values_meta = shap_values_meta[0, :, bert_pred]
                        expected_value = expected_value[bert_pred]

                    # Create Explanation object
                    explanation = shap.Explanation(
                        values=shap_values_meta,
                        base_values=expected_value,
                        data=scaled_features[0],
                        feature_names=feature_names
                    )

                    # Bar plot
                    st.write("**Feature Importance:**")
                    plt.figure()
                    shap.plots.bar(explanation, max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)

                    # Force plot
                    st.write("**Impact on Prediction:**")
                    plt.figure()
                    shap.plots.force(
                        expected_value,
                        shap_values_meta,
                        features=scaled_features[0],
                        feature_names=feature_names,
                        matplotlib=True,
                        show=False
                    )
                    st.pyplot(plt.gcf(), clear_figure=True)

                except Exception as e:
                    st.error(f"Error generating metadata explanation: {str(e)}")

if __name__ == "__main__":
    main()
   
import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
from sklearn.exceptions import NotFittedError
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler  # Import StandardScaler

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model and scaler
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str):
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stopwords.words('english'))
        features['sentiment'] = SentimentIntensityAnalyzer().polarity_scores(text)['compound']
    else:
        features['review_length'] = 0
        features['uppercase_word_count'] = 0
        features['exclamation_count'] = 0
        features['question_count'] = 0
        features['punctuation_count'] = 0
        features['avg_word_length'] = 0
        features['flesch_reading_ease'] = 0
        features['stopword_count'] = 0
        features['sentiment'] = 0
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, tokenizer)
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, vectorizer, feature_names, features_dict):
    # Prepare input data
    feature_values = np.array([[features_dict[col] for col in feature_names]])
    scaled_features = scaler.transform(feature_values)

    # Create SHAP explainer if not cached
    if 'metadata_explainer' not in st.session_state:
        st.session_state.metadata_explainer = shap.KernelExplainer(
            metadata_model.predict_proba,
            shap.sample(scaler.transform(np.zeros((1, len(feature_names))),
                        link="logit"
            )))

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
        else:
            with st.spinner("Analyzing review and generating explanations..."):
                # Make predictions
                bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
                label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"

                # Display prediction
                st.subheader(f"Prediction: {label}")
                st.write(f"**Confidence:** {confidence:.2f}")
                st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

                # BERT SHAP Explanation
                st.subheader("BERT Model Explanation")
                try:
                    shap_values = explain_with_shap(user_review, model, tokenizer)

                    # Text plot
                    st.write("**Word Importance:**")
                    shap_text_plot = shap.plots.text(shap_values[0], display=False)
                    components.html(shap_text_plot, height=300, scrolling=True)

                    # Waterfall plot
                    st.write("**Feature Impact:**")
                    plt.figure()
                    shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)
                except Exception as e:
                    st.error(f"Error generating BERT SHAP explanation: {str(e)}")

                # Metadata Analysis
                st.subheader("Metadata Analysis")
                try:
                    features = extract_metadata_features(user_review)
                    st.write("**Extracted Features:**")
                    st.json(features)

                    # Metadata SHAP Explanation
                    feature_names = list(features.keys())  # Dynamically get feature names
                    shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap(
                        metadata_model, scaler, feature_names, features
                    )

                    # Handle binary vs multi-class
                    if isinstance(shap_values_meta, list):
                        shap_values_meta = shap_values_meta[bert_pred][0]
                        expected_value = expected_value[bert_pred]
                    else:
                        shap_values_meta = shap_values_meta[0, :, bert_pred]
                        expected_value = expected_value[bert_pred]

                    # Create Explanation object
                    explanation = shap.Explanation(
                        values=shap_values_meta,
                        base_values=expected_value,
                        data=scaled_features[0],
                        feature_names=feature_names
                    )

                    # Bar plot
                    st.write("**Feature Importance:**")
                    plt.figure()
                    shap.plots.bar(explanation, max_display=10)
                    st.pyplot(plt.gcf(), clear_figure=True)

                    # Force plot
                    st.write("**Impact on Prediction:**")
                    plt.figure()
                    shap.plots.force(
                        expected_value,
                        shap_values_meta,
                        features=scaled_features[0],
                        feature_names=feature_names,
                        matplotlib=True,
                        show=False
                    )
                    st.pyplot(plt.gcf(), clear_figure=True)

                except Exception as e:
                    st.error(f"Error generating metadata explanation: {str(e)}")

if __name__ == "__main__":
    main()


import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd  # Needed for vectorizer input

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')  # Needed for SentimentIntensityAnalyzer

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, vectorizer, features_dict, bert_pred):
    # Vectorize text part (we need to extract the raw text feature from features_dict, here we don't have raw text, so we rely only on metadata)
    # In your training, you likely combined vectorized text + metadata features.
    # Since we only have metadata features here, we need to reconstruct the input vector the model expects.
    # Assume vectorizer is trained on raw text separately. So here we fake empty text for vectorizer transform.

    # But you want to combine vectorizer output + metadata features:
    # So here, you need to input the original text to vectorizer, but since only metadata features are passed,
    # We need the original review text as well (modify call accordingly).

    # For now, assume we only have metadata features (9 features) - but your model expects vectorizer + metadata (109 features).

    # So let's require the original text parameter for vectorizer.

    raise NotImplementedError("Need original review text to combine with metadata features for model input.")

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined

def explain_metadata_with_shap_complete(metadata_model, scaler, vectorizer, features_dict, text, bert_pred):
    # Prepare input vector
    input_vector = prepare_metadata_input(features_dict, vectorizer, text)
    # Scale input vector
    scaled_features = scaler.transform(input_vector)

    # Create background for KernelExplainer (use zeros with correct shape)
    if 'metadata_explainer' not in st.session_state:
        background = np.zeros((1, scaled_features.shape[1]))
        st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    # Return all
    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"
            st.subheader(f"Prediction: {label}")
            st.write(f"**Confidence:** {confidence:.2f}")
            st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

            # BERT SHAP Explanation
            st.subheader("BERT Model Explanation")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Word Importance:**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**Feature Impact (Waterfall Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Metadata Analysis")
            try:
                features = extract_metadata_features(user_review)
                st.write("**Extracted Features:**")
                st.json(features)

                # Explain metadata model with SHAP
                shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap_complete(
                    metadata_model, scaler, vectorizer, features, user_review, bert_pred
                )

                # Handle binary or multiclass output
                if isinstance(shap_values_meta, list):
                    # shap_values_meta is list of arrays for each class
                    shap_vals = shap_values_meta[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    # fallback
                    shap_vals = shap_values_meta[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get feature names for vectorizer + metadata
                # Vectorizer feature names + sorted metadata feature keys
                vectorizer_feature_names = vectorizer.get_feature_names_out()
                metadata_feature_names = sorted(features.keys())
                feature_names = list(vectorizer_feature_names) + metadata_feature_names

                # Create explanation object for plotting
                explanation = shap.Explanation(
                    values=shap_vals,
                    base_values=base_val,
                    data=scaled_features[0],
                    feature_names=feature_names
                )

                st.write("**Feature Importance (Bar Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**Impact on Prediction (Force Plot):**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    shap_vals,
                    features=scaled_features[0],
                    feature_names=feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

if __name__ == "__main__":
    main()


import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd  # Needed for vectorizer input

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')  # Needed for SentimentIntensityAnalyzer

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, vectorizer, features_dict, bert_pred):
    # Vectorize text part (we need to extract the raw text feature from features_dict, here we don't have raw text, so we rely only on metadata)
    # In your training, you likely combined vectorized text + metadata features.
    # Since we only have metadata features here, we need to reconstruct the input vector the model expects.
    # Assume vectorizer is trained on raw text separately. So here we fake empty text for vectorizer transform.

    # But you want to combine vectorizer output + metadata features:
    # So here, you need to input the original text to vectorizer, but since only metadata features are passed,
    # We need the original review text as well (modify call accordingly).

    # For now, assume we only have metadata features (9 features) - but your model expects vectorizer + metadata (109 features).

    # So let's require the original text parameter for vectorizer.

    raise NotImplementedError("Need original review text to combine with metadata features for model input.")

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    
    return text_vector, metadata_array

def explain_metadata_with_shap_complete(metadata_model, scaler, vectorizer, features_dict, text, bert_pred):
    text_vector, metadata_array = prepare_metadata_input(features_dict, vectorizer, text)
    # Prepare input vector
    combined = np.hstack([text_vector, metadata_array])
    # Scale input vector
    scaled_features = scaler.transform(combined)

    # Create background for KernelExplainer (use zeros with correct shape)
    if 'metadata_explainer' not in st.session_state:
        background = np.zeros((1, scaled_features.shape[1]))
        st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    metadata_start_idx = text_vector.shape[1]
    shap_metadata = shap_values[bert_pred][:, metadata_start_idx:]

    # Return all
    return shap_metadata, expected_value[bert_pred], metadata_array[0]

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"
            st.subheader(f"Prediction: {label}")
            st.write(f"**Confidence:** {confidence:.2f}")
            st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

            # BERT SHAP Explanation
            st.subheader("BERT Model Explanation")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Word Importance:**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**Feature Impact (Waterfall Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Metadata Analysis")
            try:
                features = extract_metadata_features(user_review)
                st.write("**Extracted Features:**")
                st.json(features)

                # Explain metadata model with SHAP
                shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap_complete(
                    metadata_model, scaler, vectorizer, features, user_review, bert_pred
                )

                # Handle binary or multiclass output
                if isinstance(shap_values_meta, list):
                    shap_vals = shap_values_meta[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    shap_vals = shap_values_meta[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get all feature names
                
                metadata_feature_names = sorted(features.keys())
                feature_names = metadata_feature_names

                # Get actual input values
                input_vals = scaled_features[0]

                # Filter out zero input features from vectorizer part to reduce noise
                filtered_feature_names = []
                filtered_input_vals = []
                filtered_shap_vals = []

                for name, val, shap_val in zip(feature_names, input_vals, shap_vals):
                    # Keep metadata or any non-zero input feature
                    if name in metadata_feature_names or abs(val) > 1e-5:
                        filtered_feature_names.append(name)
                        filtered_input_vals.append(val)
                        filtered_shap_vals.append(shap_val)

                explanation = shap.Explanation(
                    values=np.array(filtered_shap_vals),
                    base_values=base_val,
                    data=np.array(filtered_input_vals),
                    feature_names=filtered_feature_names
    )

                st.write("**Feature Importance (Bar Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**Impact on Prediction (Force Plot):**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    np.array(filtered_shap_vals),
                    features=np.array(filtered_input_vals),
                    feature_names=filtered_feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")


if __name__ == "__main__":
    main()

import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd  # Needed for vectorizer input

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')  # Needed for SentimentIntensityAnalyzer

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, vectorizer, features_dict, bert_pred):
    # Vectorize text part (we need to extract the raw text feature from features_dict, here we don't have raw text, so we rely only on metadata)
    # In your training, you likely combined vectorized text + metadata features.
    # Since we only have metadata features here, we need to reconstruct the input vector the model expects.
    # Assume vectorizer is trained on raw text separately. So here we fake empty text for vectorizer transform.

    # But you want to combine vectorizer output + metadata features:
    # So here, you need to input the original text to vectorizer, but since only metadata features are passed,
    # We need the original review text as well (modify call accordingly).

    # For now, assume we only have metadata features (9 features) - but your model expects vectorizer + metadata (109 features).

    # So let's require the original text parameter for vectorizer.

    raise NotImplementedError("Need original review text to combine with metadata features for model input.")

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined

def explain_metadata_with_shap_complete(metadata_model, scaler, vectorizer, features_dict, text, bert_pred):
    # Prepare input vector
    input_vector = prepare_metadata_input(features_dict, vectorizer, text)
    # Scale input vector
    scaled_features = scaler.transform(input_vector)

    # Create background for KernelExplainer (use zeros with correct shape)
    if 'metadata_explainer' not in st.session_state:
        background = np.zeros((1, scaled_features.shape[1]))
        st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    # Return all
    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"
            st.subheader(f"Prediction: {label}")
            st.write(f"**Confidence:** {confidence:.2f}")
            st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

            # BERT SHAP Explanation
            st.subheader("BERT Model Explanation")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Word Importance:**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**Feature Impact (Waterfall Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Metadata Analysis")
            try:
                features = extract_metadata_features(user_review)
                st.write("**Extracted Features:**")
                st.json(features)

                # Explain metadata model with SHAP
                shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap_complete(
                    metadata_model, scaler, vectorizer, features, user_review, bert_pred
                )

                vectorizer_feature_names = vectorizer.get_feature_names_out()
                vocab_size = len(vectorizer_feature_names)

                # Handle binary or multiclass output
                if isinstance(shap_values_meta, list) and len(shap_values_meta) > 1:
                    # shap_values_meta is list of arrays for each class
                    shap_vals_full = shap_values_meta[bert_pred][0]
                    base_val = expected_value[bert_pred]
                elif isinstance(shap_values_meta, list):
                    shap_vals_full = shap_values_meta[0][0]
                    base_val = expected_value[0] if isinstance(expected_value, (list, np.ndarray)) else expected_value
                else:
                    # fallback
                    shap_vals_full = shap_values_meta[0]
                    base_val = expected_value

                # Get feature names for vectorizer + metadata
                # Vectorizer feature names + sorted metadata feature keys
                shap_vals = shap_vals_full[0][:, bert_pred]
                metadata_values = np.array(scaled_features[0][vocab_size:])
                metadata_feature_names = [
                    'review_length', 'uppercase_word_count', 'exclamation_count',
                    'question_count', 'punctuation_count', 'avg_word_length',
                    'flesch_reading_ease', 'stopword_count', 'sentiment'
                ]

                # Create explanation object for plotting
                explanation = shap.Explanation(
                    values=np.array(shap_vals),
                    base_values=np.array([base_val] * len(shap_vals)),
                    data=metadata_values,
                    feature_names=metadata_feature_names
                )

                st.write("**Feature Importance (Bar Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**Impact on Prediction (Force Plot):**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    shap_vals,
                    metadata_values,
                    feature_names=metadata_feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

if __name__ == "__main__":
    main()





import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd  # Needed for vectorizer input

# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')  # Needed for SentimentIntensityAnalyzer

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

# Explain metadata model with SHAP
def explain_metadata_with_shap(metadata_model, scaler, vectorizer, features_dict, bert_pred):
    # Vectorize text part (we need to extract the raw text feature from features_dict, here we don't have raw text, so we rely only on metadata)
    # In your training, you likely combined vectorized text + metadata features.
    # Since we only have metadata features here, we need to reconstruct the input vector the model expects.
    # Assume vectorizer is trained on raw text separately. So here we fake empty text for vectorizer transform.

    # But you want to combine vectorizer output + metadata features:
    # So here, you need to input the original text to vectorizer, but since only metadata features are passed,
    # We need the original review text as well (modify call accordingly).

    # For now, assume we only have metadata features (9 features) - but your model expects vectorizer + metadata (109 features).

    # So let's require the original text parameter for vectorizer.

    raise NotImplementedError("Need original review text to combine with metadata features for model input.")

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined

def explain_metadata_with_shap_complete(metadata_model, scaler, vectorizer, features_dict, text, bert_pred):
    # Prepare input vector
    input_vector = prepare_metadata_input(features_dict, vectorizer, text)
    # Scale input vector
    scaled_features = scaler.transform(input_vector)

    # Create background for KernelExplainer (use zeros with correct shape)
    if 'metadata_explainer' not in st.session_state:
        background = np.zeros((1, scaled_features.shape[1]))
        st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

    # Calculate SHAP values
    shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
    expected_value = st.session_state.metadata_explainer.expected_value

    # Return all
    return shap_values, expected_value, scaled_features

# Main app
def main():
    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review (CG)" if bert_pred == 1 else "üü¢ Genuine Review (OR)"
            st.subheader(f"Prediction: {label}")
            st.write(f"**Confidence:** {confidence:.2f}")
            st.write(f"**Probabilities:** Fake: {bert_probs[1]:.2f} | Genuine: {bert_probs[0]:.2f}")

            # BERT SHAP Explanation
            st.subheader("BERT Model Explanation")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Word Importance:**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**Feature Impact (Waterfall Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Metadata Analysis")
            try:
                features = extract_metadata_features(user_review)
                st.write("**Extracted Features:**")
                st.json(features)

                # Explain metadata model with SHAP
                shap_values_meta, expected_value, scaled_features = explain_metadata_with_shap_complete(
                    metadata_model, scaler, vectorizer, features, user_review, bert_pred
                )

                # Handle binary or multiclass output
                if isinstance(shap_values_meta, list):
                    # shap_values_meta is list of arrays for each class
                    shap_vals = shap_values_meta[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    # fallback
                    shap_vals = shap_values_meta[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get feature names for vectorizer + metadata
                # Vectorizer feature names + sorted metadata feature keys
                vectorizer_feature_names = vectorizer.get_feature_names_out()
                metadata_feature_names = sorted(features.keys())
                feature_names = list(vectorizer_feature_names) + metadata_feature_names

                # Create explanation object for plotting
                explanation = shap.Explanation(
                    values=shap_vals,
                    base_values=base_val,
                    data=scaled_features[0],
                    feature_names=feature_names
                )

                st.write("**Feature Importance (Bar Plot):**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**Impact on Prediction (Force Plot):**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    shap_vals,
                    features=scaled_features[0],
                    feature_names=feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

if __name__ == "__main__":
    main()
    """
import streamlit as st
import torch
from transformers import BertForSequenceClassification, BertTokenizer
import shap
import joblib
import numpy as np
import nltk
import matplotlib.pyplot as plt
import streamlit.components.v1 as components
import warnings
import string
from nltk.corpus import stopwords
import textstat
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd
import os
import gdown
import zipfile

# --- Load models with caching ---
@st.cache_resource
def load_bert_model():
    model_path = "models/bert"

    if not os.path.exists(model_path):
        os.makedirs(model_path, exist_ok=True)
        url = "https://drive.google.com/uc?id=1jIuOqN-K8rR4RcCy5Ho0yHL3jd5WL9s1"
        output = "models/bert_model.zip"
        gdown.download(url, output, quiet=False)
        with zipfile.ZipFile(output, 'r') as zip_ref:
            zip_ref.extractall(model_path)

    model = BertForSequenceClassification.from_pretrained(model_path, local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)
    return model, tokenizer

@st.cache_resource
def load_metadata_model():
    model_path = "models/metadata"

    if not os.path.exists(model_path):
        os.makedirs(model_path, exist_ok=True)
        url = "https://drive.google.com/uc?id=1glZNtinueDjvF68QEz6MA2McgAq7iAda"
        output = "models/metadata_model.zip"
        gdown.download(url, output, quiet=False)
        with zipfile.ZipFile(output, 'r') as zip_ref:
            zip_ref.extractall(model_path)

    model = joblib.load(os.path.join(model_path, "metadata_model.joblib"))
    return model


# Suppress warnings
warnings.filterwarnings("ignore")

# Initialize NLTK
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Page setup
st.set_page_config(page_title="Fake Review Detection", layout="wide")

# Load BERT model/tokenizer
@st.cache_resource
def load_bert_model():
    model = BertForSequenceClassification.from_pretrained("models", local_files_only=True)
    tokenizer = BertTokenizer.from_pretrained("models", local_files_only=True)
    return model, tokenizer

stop_words = set(stopwords.words('english'))
sia = SentimentIntensityAnalyzer()

# Load metadata model, scaler, vectorizer
@st.cache_resource
def load_metadata_model():
    metadata_model = joblib.load("metadata_model/metadata_model.joblib")
    scaler = joblib.load("scaler/metadata_scaler.joblib")
    vectorizer = joblib.load("vectorizer/metadata_vectorizer.joblib")
    return metadata_model, scaler, vectorizer

# Extract metadata features
def extract_metadata_features(text):
    features = {}
    if isinstance(text, str) and text.strip():
        words = text.split()
        features['review_length'] = len(words)
        features['uppercase_word_count'] = sum(1 for word in words if word.isupper())
        features['exclamation_count'] = text.count('!')
        features['question_count'] = text.count('?')
        features['punctuation_count'] = sum(1 for char in text if char in string.punctuation)
        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0
        features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)
        features['stopword_count'] = sum(1 for word in text.lower().split() if word in stop_words)
        features['sentiment'] = sia.polarity_scores(text)['compound']
    else:
        features = {key: 0 for key in [
            'review_length', 'uppercase_word_count', 'exclamation_count', 'question_count',
            'punctuation_count', 'avg_word_length', 'flesch_reading_ease', 'stopword_count', 'sentiment'
        ]}
    return features

# Predict using BERT
def predict_bert(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs).item()
        confidence = float(probs[0][pred])
    return pred, confidence, probs.cpu().numpy()[0]

# Explain with SHAP (BERT)
def explain_with_shap(text, model, tokenizer):
    def f(x):
        # x is a list of texts
        inputs = tokenizer(list(x), return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs).logits
            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
        return probs

    explainer = shap.Explainer(f, masker=shap.maskers.Text(tokenizer))
    shap_values = explainer([text])
    return shap_values

def prepare_metadata_input(metadata_features, vectorizer, text):
    # Vectorize text (must be a list)
    text_vector = vectorizer.transform([text]).toarray()
    # Metadata features as array (shape: 1,9)
    metadata_array = np.array([[metadata_features[col] for col in sorted(metadata_features.keys())]])
    # Combine vectorized text and metadata features horizontally
    combined = np.hstack([text_vector, metadata_array])
    return combined, text_vector, metadata_array

# Load models
bert_model, bert_tokenizer = load_bert_model()
metadata_model = load_metadata_model()

# Main app
def main():


    st.title("üß† Fake Review Detection System")
    st.markdown("Enter a product review to determine if it's **Fake** or **Genuine**.")

    model, tokenizer = load_bert_model()
    metadata_model, scaler, vectorizer = load_metadata_model()

    if "reset" not in st.session_state:
        st.session_state.reset = False
    if st.session_state.reset:
        st.session_state.reset = False
        st.experimental_rerun()    

    user_review = st.text_area("Enter your review:", "", height=150)

    if st.button("Analyze Review"):
        if not user_review.strip():
            st.warning("Please enter a review.")
            return
        with st.spinner("Analyzing review and generating explanations..."):
            # BERT Prediction
            bert_pred, confidence, bert_probs = predict_bert(user_review, model, tokenizer)
            label = "üî¥ Fake Review " if bert_pred == 1 else "üü¢ Genuine Review "
            st.subheader(f"Prediction: {label}")

            features = extract_metadata_features(user_review)
            combined_input, text_vector, metadata_array = prepare_metadata_input(
                features, vectorizer, user_review
            )
            scaled_features = scaler.transform(combined_input)
            meta_probs = metadata_model.predict_proba(scaled_features)

            bert_prob_genuine = bert_probs[1]
            meta_prob_genuine = meta_probs[0][1]
            ensemble_prob_genuine = (bert_prob_genuine + meta_prob_genuine) / 2.0
            ensemble_pred = 1 if ensemble_prob_genuine > 0.5 else 0
            ensemble_percent = round(ensemble_prob_genuine * 100, 2)
            st.markdown(f"**üí° Confidence Behind the Answer:** {ensemble_percent}%")

            # Visual progress bar
            st.progress(int(ensemble_percent))

            if ensemble_percent >= 70:
                st.success("üåü Highly Confident Prediction")
            elif ensemble_percent >= 50:    
                st.info("üîé Moderately Confident Prediction")
            else:
                st.warning("‚ö†Ô∏è Low Confidence ‚Äî Review borderline or unclear")    

            # BERT SHAP Explanation
            st.subheader("How It Interprets Your Review")
            try:
                shap_values = explain_with_shap(user_review, model, tokenizer)

                # Text plot
                st.write("**Which Words Mattered Most?**")
                shap_text_plot = shap.plots.text(shap_values[0], display=False)
                components.html(shap_text_plot, height=300, scrolling=True)

                # Waterfall plot for predicted class
                st.write("**See the Step-by-Step Decision Path**")
                plt.figure(figsize=(10,5))
                shap.plots.waterfall(shap_values[0, :, bert_pred], max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)
            except Exception as e:
                st.error(f"Error generating BERT SHAP explanation: {e}")

            # Metadata Analysis and Explanation
            st.subheader("Digging Deeper: Extra Clues in Your Review")
            try:
                features = extract_metadata_features(user_review)
                st.markdown("**Review Metadata Summary:**")
                for key, value in features.items():
                    st.markdown(f"- **{key.replace('_', ' ').title()}**: `{value}`")

                # Get combined input and separate components
                combined_input, text_vector, metadata_array = prepare_metadata_input(
                    features, vectorizer, user_review
                )
                
                # Scale input vector
                scaled_features = scaler.transform(combined_input)

                # Create background for KernelExplainer (use zeros with correct shape)
                if 'metadata_explainer' not in st.session_state:
                    background = np.zeros((1, scaled_features.shape[1]))
                    st.session_state.metadata_explainer = shap.KernelExplainer(metadata_model.predict_proba, background)

                # Calculate SHAP values
                shap_values = st.session_state.metadata_explainer.shap_values(scaled_features)
                expected_value = st.session_state.metadata_explainer.expected_value

                # Handle binary or multiclass output
                if isinstance(shap_values, list):
                    # shap_values is list of arrays for each class
                    shap_vals = shap_values[bert_pred][0]
                    base_val = expected_value[bert_pred]
                else:
                    # fallback
                    shap_vals = shap_values[0, :, bert_pred]
                    base_val = expected_value[bert_pred]

                # Get only the metadata feature names and their SHAP values
                num_text_features = text_vector.shape[1]  # Number of text features from vectorizer
                metadata_shap_vals = shap_vals[num_text_features:]  # Only take SHAP values for metadata features
                metadata_scaled_features = scaled_features[0, num_text_features:]  # Only scaled metadata features
                
                # Sorted metadata feature names
                metadata_feature_names = sorted(features.keys())

                # Create explanation object for plotting (metadata only)
                explanation = shap.Explanation(
                    values=metadata_shap_vals,
                    base_values=base_val,
                    data=metadata_scaled_features,
                    feature_names=metadata_feature_names
                )

                st.write("**Top Metadata Clues Behind the Prediction**")
                plt.figure(figsize=(10,5))
                shap.plots.bar(explanation, max_display=15)
                st.pyplot(plt.gcf(), clear_figure=True)

                st.write("**How Each Clue Pushed the Prediction**")
                plt.figure(figsize=(10,3))
                shap.plots.force(
                    base_val,
                    metadata_shap_vals,
                    features=metadata_scaled_features,
                    feature_names=metadata_feature_names,
                    matplotlib=True,
                    show=False
                )
                st.pyplot(plt.gcf(), clear_figure=True)

            except Exception as e:
                st.error(f"Error generating metadata explanation: {e}")

        if st.button("üîÑ Try Another Review"):
            st.session_state.user_review = ""
            st.experimental_rerun()

if __name__ == "__main__":
    main()