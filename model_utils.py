# -*- coding: utf-8 -*-
"""Fake reviews detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBaUeTItp9i79zDV2lx0s5Tz8CQYs6TR
"""
# Paths to the model and scaler files
from transformers import BertTokenizer, BertForSequenceClassification
import torch
torch.cuda.is_available()
import joblib 
import pandas as pd # type: ignore
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns # type: ignore
import nltk
import re
import contractions
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag

#from google.colab import drive
#drive.mount('/content/gdrive')
BERT_MODEL_DIR = './models'
SCALER_PATH = 'scaler/metadata_scaler.joblib'
METADATA_MODEL_PATH = 'metadata_model/metadata_model.joblib'

# Load the models and scaler
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)
bert_model = BertForSequenceClassification.from_pretrained(BERT_MODEL_DIR)
scaler = joblib.load(SCALER_PATH)
metadata_model = joblib.load(METADATA_MODEL_PATH)
#!pip install contractions

# Download NLTK Data (One-time setup)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')
try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')

# Foldername = '/content/gdrive/My Drive/'

# Remove or comment this line:
# Foldername = '/content/gdrive/My Drive/'

# Replace with the full local path to your dataset:
df = pd.read_csv('C:/Users/kbcmo/Downloads/fake reviews dataset.csv', on_bad_lines='skip')

# Initial Data Exploration
print("Dataset Head:\n", df.head())
print("\nDataset Info:\n", df.info())
print("\nCategory Distribution (Before Filtering):\n", df['category'].value_counts())
print("\nLabel Distribution:\n", df['label'].value_counts())
print("\nRating Distribution:\n", df['rating'].value_counts())

# Data Visualization
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df)
plt.title('Label Distribution')
plt.show()

if 'category' in df.columns:
    plt.figure(figsize=(12, 8))
    sns.countplot(x='category', data=df)
    plt.title('Category Distribution')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(8, 6))
df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
df.dropna(subset=['rating'], inplace=True)
sns.histplot(df['rating'], kde=True)
plt.title('Rating Distribution')
plt.show()

# Remove Duplicates
df.drop_duplicates(inplace=True)
print("\nDuplicates removed. New dataset size:", df.shape)

# Handle Missing Values
print("\nMissing Values Before Handling:\n", df.isnull().sum())
df.fillna({'rating': df['rating'].mode()[0], 'text_': ''}, inplace=True)
print("\nMissing Values After Handling:\n", df.isnull().sum())

# Filter Product-Related Reviews
product_categories = [
    "Pet_Supplies_5", "Home_and_Kitchen_5", "Electronics_5",
    "Sports_and_Outdoors_5", "Tools_and_Home_Improvement_5",
    "Clothing_Shoes_and_Jewelry_5", "Toys_and_Games_5", "Beauty_and_Personal_Care_5"
]

if 'category' in df.columns:
    df = df[df['category'].isin(product_categories)]
    print("\nNon-product reviews removed.")
    print("\nUpdated Category Distribution:\n", df['category'].value_counts())

# Data Visualization (After Filtering)
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df)
plt.title('Label Distribution (Filtered)')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='category', data=df)
plt.title('Category Distribution (Filtered)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['rating'], kde=True)
plt.title('Rating Distribution (Filtered)')
plt.show()

import nltk
nltk.download('averaged_perceptron_tagger')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    if isinstance(text, str):
        text = contractions.fix(text)  # Expand contractions
        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
        text = re.sub(r'[^a-zA-Z\s]', '', text)  # Keep only letters and spaces
        text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
        text = text.lower()  # Convert to lowercase
        words = word_tokenize(text)  # Tokenize words
        words = [word for word in words if word not in stop_words]  # Remove stopwords
        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]  # Lemmatization with POS tagging
        return ' '.join(words)  # Convert list back to string
    return ""

# Function for POS tagging (better lemmatization)
def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": 'a', "N": 'n', "V": 'v', "R": 'r'}
    return tag_dict.get(tag, 'n')

import nltk
nltk.download('averaged_perceptron_tagger_eng')
# Apply Cleaning Function
if 'text_' in df.columns:
    df['text_'] = df['text_'].apply(clean_text)
    df = df[df['text_'] != ""]  # Remove empty text rows
else:
    print("Column 'text_' not found in dataset!")

# Handle Missing Values
print("\nMissing Values Before Handling:\n", df.isnull().sum())

df.loc[:, 'rating'] = df['rating'].fillna(df['rating'].mode()[0])
df.loc[:, 'text_'] = df['text_'].fillna("")

print("\nMissing Values After Handling:\n", df.isnull().sum())

df.to_csv('C:/Users/kbcmo/Downloads/cleaned_reviews(15).csv', index=False)

print("\nData cleaning and exploration complete.  Dataset saved to Google Drive as 'cleaned_reviews.csv'.")

print(df['label'].value_counts())

# Automatically download the file
#from google.colab import files
#files.download('/content/gdrive/My Drive/cleaned_reviews.csv')

# Phase 2: Feature Engineering and Selection

#from sklearn.feature_extraction.text import TfidfVectorizer
#from sklearn.preprocessing import StandardScaler
#from textblob import TextBlob
#import nltk
#from nltk.tokenize import word_tokenize
#from nltk import pos_tag
#import numpy as np  # Import numpy for array manipulation
#from scipy.sparse import hstack # For combining sparse matrices

# Ensure NLTK POS Tagging resources are available
#nltk.download('averaged_perceptron_tagger')

# --- Feature Engineering ---

# TF-IDF Vectorization
#vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
#X_tfidf = vectorizer.fit_transform(df['text_'])

# Metadata Features
#df['review_length'] = df['text_'].apply(lambda x: len(x.split()))  # Word count
#df['num_exclamation'] = df['text_'].apply(lambda x: x.count('!'))  # Count '!'
#df['num_question'] = df['text_'].apply(lambda x: x.count('?'))  # Count '?'
#df['num_uppercase'] = df['text_'].apply(lambda x: sum(1 for c in x if c.isupper()))  # Uppercase letters count

#def count_pos_tags(text):
 #   words = word_tokenize(text)
  #  pos_counts = pos_tag(words)
   # noun_count = sum(1 for _, tag in pos_counts if tag.startswith('NN'))
    #verb_count = sum(1 for _, tag in pos_counts if tag.startswith('VB'))
    #return noun_count, verb_count

#import nltk
#nltk.download('averaged_perceptron_tagger_eng')

# POS Tagging Features
#df['noun_count'], df['verb_count'] = zip(*df['text_'].apply(count_pos_tags))

#def get_sentiment_score(text):
 #   return TextBlob(text).sentiment.polarity

# Sentiment Score Feature
#df['sentiment_score'] = df['text_'].apply(get_sentiment_score)

# Scale Numerical Features
#scaler = StandardScaler()
#num_features = ['review_length', 'num_exclamation', 'num_question', 'num_uppercase', 'noun_count', 'verb_count', 'sentiment_score']
#df[num_features] = scaler.fit_transform(df[num_features])

# --- Feature Combination ---
#X_numerical = df[num_features].values  # Get the scaled numerical features as a NumPy array

# Combine TF-IDF and numerical features
#X = hstack([X_tfidf, X_numerical])  # Use hstack for sparse matrices

# --- Feature Selection (Example using SelectKBest) ---
#from sklearn.feature_selection import SelectKBest, f_classif

# Select top K features (adjust K as needed)
#k = 1000  # Example value; you'll need to experiment with different values
#selector = SelectKBest(f_classif, k=k)
#y = df['label'] # Assuming 'label' is your target variable
#X_new = selector.fit_transform(X, y)

# Get selected feature names (for TF-IDF features)
#tfidf_feature_names = vectorizer.get_feature_names_out()
#selected_tfidf_features = tfidf_feature_names[selector.get_support()[:X_tfidf.shape[1]]] # Up to number of TF-IDF features

# Get selected numerical feature names
#selected_numerical_indices = np.where(selector.get_support()[X_tfidf.shape[1:]])[0]
#selected_numerical_features = [num_features[i] for i in selected_numerical_indices] # From number of TF-IDF features onwards

# Combine selected feature names
#selected_feature_names = np.concatenate([selected_tfidf_features, selected_numerical_features])

# --- Save Feature Matrix and Selected Features ---
#import joblib  # For saving the feature matrix and selected features

# Save the combined feature matrix
#joblib.dump(X_new, 'feature_matrix.pkl')

# Save the list of selected features
#joblib.dump(selected_feature_names, 'selected_features.pkl')

#print("Phase 2 complete! Features engineered and selected. Feature matrix and selected features saved.")

print(df['label'].value_counts())  # Check the distribution before encoding

print(df['label'].unique())  # Check unique values in the label column before encoding

# Encode labels: 'CG' -> 1, 'OR' -> 0
df['label'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)

# Check the label distribution after encoding
print("Label Distribution After Encoding:")
print(df['label'].value_counts())

from sklearn.model_selection import train_test_split

# Split into train and validation sets (80%-20%)
X = df['text_']  # Feature: preprocessed reviews
y = df['label']  # Target: encoded labels (CG=1, OR=0)

# First split: Train+Val and Test (80%-20%)
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split: Train and Val from remaining (80% of 80% = 64% train, 16% val)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)

# Verifications
print(f"Train Set: {len(X_train)} samples")
print(f"Validation Set: {len(X_val)} samples")
print(f"Test Set: {len(X_test)} samples")

print("\nTraining Label Distribution:")
print(y_train.value_counts())

print("\nValidation Label Distribution:")
print(y_val.value_counts())

print("\nTest Label Distribution:")
print(y_test.value_counts())

#!pip install textstat

#metadata features

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
import string
import textstat
from nltk.corpus import stopwords

# Download necessary NLTK data (only if you haven't done so)
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:  # Use LookupError instead of nltk.downloader.DownloadError
    nltk.download('vader_lexicon')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Initialize Sentiment Analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to get the compound sentiment score
def get_sentiment(text):
    if isinstance(text, str):
        return analyzer.polarity_scores(text)['compound']
    return 0.0

# Apply sentiment analysis
df['sentiment'] = df['text_'].apply(get_sentiment)
print("Sentiment Scores:\n", df[['text_', 'sentiment']].head())

# --- Word Frequency Features ---
vectorizer = CountVectorizer(max_features=100, stop_words='english')
word_counts = vectorizer.fit_transform(df['text_']).toarray()
feature_names = vectorizer.get_feature_names_out()
word_count_df = pd.DataFrame(word_counts, columns=feature_names)
df = pd.concat([df, word_count_df], axis=1)
print("Top words used in training:", feature_names.tolist())
print("\nWord Frequency Features:\n", df[feature_names].head())

# --- Additional Metadata Features ---
df['review_length'] = df['text_'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)
df['uppercase_word_count'] = df['text_'].apply(lambda x: sum(1 for word in x.split() if word.isupper()) if isinstance(x, str) else 0)
df['exclamation_count'] = df['text_'].apply(lambda x: x.count('!') if isinstance(x, str) else 0)
df['question_count'] = df['text_'].apply(lambda x: x.count('?') if isinstance(x, str) else 0)
df['punctuation_count'] = df['text_'].apply(lambda x: sum(1 for char in x if char in string.punctuation) if isinstance(x, str) else 0)
df['avg_word_length'] = df['text_'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if isinstance(x, str) and len(x.split()) > 0 else 0)

# Install textstat if you haven't
#!pip install textstat
def get_flesch_reading_ease(text):
    if isinstance(text, str):
        try:
            return textstat.flesch_reading_ease(text)
        except ZeroDivisionError:
            return 0
    return 0
df['flesch_reading_ease'] = df['text_'].apply(get_flesch_reading_ease)

def count_stopwords(text):
    if isinstance(text, str):
        words = text.lower().split()
        return sum(1 for word in words if word in stopwords.words('english'))
    return 0
df['stopword_count'] = df['text_'].apply(count_stopwords)

print("\nAdditional Metadata Features added.")
print(df[['text_', 'sentiment', 'review_length', 'uppercase_word_count', 'flesch_reading_ease']].head())

# Save the DataFrame with metadata features if needed
# df.to_csv('/content/gdrive/My Drive/reviews_with_metadata.csv', index=False)

from transformers import BertTokenizer

# Load pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenizing the training and validation data
train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)


print("Checking tokenized outputs...")
print(train_encodings.keys())  # Should print dict_keys(['input_ids', 'attention_mask'])

# Ensure labels are integers
print("Checking label data types...")
print(y_train.dtype, y_val.dtype)  # Should be int64

# Confirm no empty sequences
print("Checking sequence lengths...")
print(len(train_encodings['input_ids']), len(train_encodings['attention_mask']))
print(len(val_encodings['input_ids']), len(val_encodings['attention_mask']))

# Convert the tokenized data into tensors (for BERT)
import torch
train_dataset = torch.utils.data.TensorDataset(
    torch.tensor(train_encodings['input_ids']),
    torch.tensor(train_encodings['attention_mask']),
    torch.tensor(y_train.values)
)

val_dataset = torch.utils.data.TensorDataset(
    torch.tensor(val_encodings['input_ids']),
    torch.tensor(val_encodings['attention_mask']),
    torch.tensor(y_val.values)
)

test_dataset = torch.utils.data.TensorDataset(
    torch.tensor(test_encodings['input_ids']),
    torch.tensor(test_encodings['attention_mask']),
    torch.tensor(y_test.values)
)

# Model Setup

#!pip install --upgrade jax jaxlib flax optax
#!pip install --upgrade tensorflow
#!pip install --upgrade transformers

import torch
from transformers import BertForSequenceClassification
from torch.optim import AdamW
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os

# Check if GPU is available, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Prepare DataLoader for training and validation datasets
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Load pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.to(device)  # Move model to GPU (if available)

# Initialize optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)

import matplotlib.pyplot as plt

# Track losses for graphing
train_losses = []
val_losses = []

# Early stopping parameters (for tracking, not stopping early)
patience = 5  # Adjust as needed
best_val_loss = float('inf')
epochs_no_improve = 0
num_epochs = 50  # Setting the required number of epochs
best_model_state = None  # To store the state dict of the best model
best_epoch = -1

# Learning rate scheduler setup (optional but can help with convergence)
from transformers import get_scheduler

# Set up a scheduler
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_epochs * len(train_dataloader)
)

# output_dir = '/content/gdrive/MyDrive/best_bert_model_50epochs'

'''
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize the scaler for mixed precision training
scaler = GradScaler()

# Training loop with early stopping
for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    optimizer.zero_grad()
    accumulation_steps = 4  # Adjust as needed

    for step, batch in enumerate(train_dataloader):
       input_ids = batch[0].to(device)
       attention_mask = batch[1].to(device)
       labels = batch[2].to(device)

       with autocast():  # Enable mixed precision
           # Forward pass
           outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
           loss = outputs.loss
           total_train_loss += loss.item()

           # Backward pass (gradient accumulation)
       scaler.scale(loss).backward()

       # Gradient accumulation
       if (step + 1) % accumulation_steps == 0:
           scaler.step(optimizer)
           scaler.update()
           optimizer.zero_grad()

    avg_train_loss = total_train_loss / len(train_dataloader)
    train_losses.append(avg_train_loss)
    print(f"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss}")

      # Step the learning rate scheduler
    lr_scheduler.step()

    Epoch 1/50, Training Loss: 0.10487509106765215
Epoch 2/50, Training Loss: 0.0404584721594073
Epoch 3/50, Training Loss: 0.02081984210224536
Epoch 4/50, Training Loss: 0.012352480555436474
Epoch 5/50, Training Loss: 0.009979710907156882
Epoch 6/50, Training Loss: 0.0062675527246132505
Epoch 7/50, Training Loss: 0.005712658994206353
Epoch 8/50, Training Loss: 0.004046223721428642
Epoch 9/50, Training Loss: 0.0042426635117890155
Epoch 10/50, Training Loss: 0.003093474936350167
Epoch 11/50, Training Loss: 0.003145008264258066
Epoch 12/50, Training Loss: 0.00335685901756701
Epoch 13/50, Training Loss: 0.00251905751155766
Epoch 14/50, Training Loss: 0.0024604877202101458
Epoch 15/50, Training Loss: 0.0030384960167554125
Epoch 16/50, Training Loss: 0.002124410194573935
Epoch 17/50, Training Loss: 0.0013189999860540144
Epoch 18/50, Training Loss: 0.0024770016646705486
Epoch 19/50, Training Loss: 0.0027039007953892826
Epoch 20/50, Training Loss: 0.0018489857780568046
Epoch 21/50, Training Loss: 0.0015056860530756784
Epoch 22/50, Training Loss: 0.0018682371649819196
Epoch 23/50, Training Loss: 0.0017430947094179822
Epoch 24/50, Training Loss: 0.0015875724379651992
Epoch 25/50, Training Loss: 0.002440456479262345
Epoch 26/50, Training Loss: 0.0021401432168448704
Epoch 27/50, Training Loss: 0.0011131095783636412
Epoch 28/50, Training Loss: 0.0013297929340155559
Epoch 29/50, Training Loss: 0.0010493813171720025
Epoch 30/50, Training Loss: 0.0018907332203275111
Epoch 31/50, Training Loss: 0.0011477812336434044
Epoch 32/50, Training Loss: 0.0012460348362886636
Epoch 33/50, Training Loss: 0.0014487552768562542
Epoch 34/50, Training Loss: 0.001189126407119229
Epoch 35/50, Training Loss: 0.0006200481412617926
Epoch 36/50, Training Loss: 0.0015728984819451466
Epoch 37/50, Training Loss: 0.0010877473171922363
Epoch 38/50, Training Loss: 0.0011798665396662742
Epoch 39/50, Training Loss: 0.0009006416639367839
Epoch 40/50, Training Loss: 0.0011333088577419243
Epoch 41/50, Training Loss: 0.0006636984499310566
Epoch 42/50, Training Loss: 0.0012398314960207087
Epoch 43/50, Training Loss: 0.0009394964464415431
Epoch 44/50, Training Loss: 0.001389884457968044
Epoch 45/50, Training Loss: 0.0011304706338195617
Epoch 46/50, Training Loss: 0.0005221593261740971
Epoch 47/50, Training Loss: 0.0007102918333360671
Epoch 48/50, Training Loss: 0.0013407410992783324
Epoch 49/50, Training Loss: 0.000912164958032813
Epoch 50/50, Training Loss: 0.0008783227647556967

'''

'''
    # Validation
    model.eval()
    total_eval_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in val_dataloader:
            # Move batch tensors to the same device as the model
            input_ids = batch[0].to(device)
            attention_mask = batch[1].to(device)
            labels = batch[2].to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_eval_loss += loss.item()

            preds = torch.argmax(logits, dim=-1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_eval_loss = total_eval_loss / len(val_dataloader)
    val_losses.append(avg_eval_loss)
    print(f"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_eval_loss}")

    # Evaluate metrics on validation set
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds)
    recall = recall_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)

    print(f"Validation Accuracy: {accuracy:.4f}")
    print(f"Validation Precision: {precision:.4f}")
    print(f"Validation Recall: {recall:.4f}")
    print(f"Validation F1-Score: {f1:.4f}")

    # Early stopping check
    if avg_eval_loss < best_val_loss:
        best_val_loss = avg_eval_loss
        epochs_no_improve = 0
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print(f"Best model saved at epoch {epoch + 1}")
    else:
        epochs_no_improve += 1
        print(f"Epochs with no improvement: {epochs_no_improve}")
        if epochs_no_improve >= patience:
            print("Early stopping would trigger here, but continuing until 50 epochs.")
        # We are NOT stopping early here

# Load the best model
model = BertForSequenceClassification.from_pretrained(output_dir)
tokenizer = BertTokenizer.from_pretrained(output_dir)
model.to(device)
print("Best model loaded.")

# --- Plot Training vs Validation Loss ---
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')
plt.hlines(val_losses[0], xmin=1, xmax=num_epochs, colors='orange', label='Final Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss & Final Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

print("Model training complete.")

output: Epoch 50/50, Validation Loss: 0.11643182823543437

Validation Accuracy: 0.9833

Validation Precision: 0.9811

Validation Recall: 0.9857

Validation F1-Score: 0.9834

Best model saved at epoch 50

Best model loaded.

'''

#!zip -r best_model.zip "/content/gdrive/MyDrive/best_bert_model_50epochs"

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from torch.utils.data import DataLoader
# Load best model and tokenizer (already trained and saved)
from transformers import BertForSequenceClassification, BertTokenizer

BERT_MODEL_DIR = './models'

tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)
bert_model = BertForSequenceClassification.from_pretrained(BERT_MODEL_DIR)

model.to(device)
model.eval()

test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# --- Evaluate on Test Set ---
total_test_loss = 0
all_test_preds = []
all_test_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

        total_test_loss += loss.item()

        preds = torch.argmax(logits, dim=-1)
        all_test_preds.extend(preds.cpu().numpy())
        all_test_labels.extend(labels.cpu().numpy())

avg_test_loss = total_test_loss / len(test_dataloader)
accuracy = accuracy_score(all_test_labels, all_test_preds)
precision = precision_score(all_test_labels, all_test_preds)
recall = recall_score(all_test_labels, all_test_preds)
f1 = f1_score(all_test_labels, all_test_preds)

# Print Evaluation Results
print("\n=== Final Evaluation on Test Set ===")
print(f"Test Loss: {avg_test_loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Precision: {precision:.4f}")
print(f"Test Recall: {recall:.4f}")
print(f"Test F1-Score: {f1:.4f}")
print("\nClassification Report:")
print(classification_report(all_test_labels, all_test_preds, digits=4))

#from google.colab import drive
import numpy as np
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import pandas as pd
'''
drive.mount('/content/gdrive')

GDRIVE_PATH = '/content/gdrive/My Drive/'

# Load your dataset (make sure it has a column called 'text_')
METADATA_CSV_PATH = os.path.join(GDRIVE_PATH, 'reviews_with_metadata.csv')

# Path to save/load the scaler object
SCALER_PATH = os.path.join(GDRIVE_PATH, 'metadata_scaler.joblib')

# Path to save/load the trained model
METADATA_MODEL_PATH = os.path.join(GDRIVE_PATH, 'metadata_model.joblib')

df = pd.read_csv('/content/gdrive/My Drive/reviews_with_metadata.csv') # Load the DataFrame
'''
# Define local paths instead

DATA_DIR = os.path.join(os.getcwd(), 'data')
MODEL_DIR = os.path.join(os.getcwd(), 'models')

METADATA_CSV_PATH = os.path.join(DATA_DIR, 'reviews_with_metadata.csv')
SCALER_PATH = os.path.join(MODEL_DIR, 'metadata_scaler.joblib')
METADATA_MODEL_PATH = os.path.join(MODEL_DIR, 'metadata_model.joblib')
# BERT_MODEL_DIR = os.path.join(MODEL_DIR, 'best_bert_model_50epochs')

# df = pd.read_csv(METADATA_CSV_PATH)

#df.head()

#print(df.columns)


# 1.2 Load Data
print(f"Loading data with metadata features from: {METADATA_CSV_PATH}")
try:
    metadata_df = pd.read_csv(METADATA_CSV_PATH)
    # Ensure 'label' column exists and handle potential NaNs in metadata
    if 'label' not in metadata_df.columns:
        raise ValueError("DataFrame must contain a 'label' column.")
    metadata_df.dropna(subset=['label'], inplace=True) # Remove rows where label is missing
    metadata_df['label'] = metadata_df['label'].astype(int) # Ensure label is integer

    # Handle potential NaNs in metadata columns robustly (e.g., fill with median)
    numeric_cols = metadata_df.select_dtypes(include=np.number).columns.tolist()
    if 'label' in numeric_cols: numeric_cols.remove('label') # Don't fill labels

    print(f"Checking for NaNs in numeric metadata columns: {numeric_cols}")
    nan_counts_before = metadata_df[numeric_cols].isnull().sum()
    if nan_counts_before.sum() > 0:
        print("NaNs found, filling with median...")
        metadata_df[numeric_cols] = metadata_df[numeric_cols].fillna(metadata_df[numeric_cols].median())
        print("NaNs filled.")
    else:
        print("No NaNs found in numeric metadata columns.")

    print(f"Data loaded successfully. Shape: {metadata_df.shape}")
except FileNotFoundError:
    print(f"ERROR: Could not find metadata CSV at {METADATA_CSV_PATH}")
    exit() # Stop if file not found
except Exception as e:
    print(f"An error occurred during data loading: {e}")
    exit()

# 1.3 Define Features (Metadata ONLY) and Target
print("Defining features (metadata only) and target...")

fixed_metadata_features = [
    'sentiment',
    'review_length',
    'uppercase_word_count',
    'exclamation_count',
    'question_count',
    'punctuation_count',
    'avg_word_length',
    'flesch_reading_ease',
    'stopword_count'
]

    # Dynamically get the word frequency feature columns
all_columns = metadata_df.columns.tolist()
start_idx = all_columns.index('addition')
end_idx = all_columns.index('year') + 1  # +1 to include 'year'
word_freq_features = all_columns[start_idx:end_idx]

    # Combine both sets of features
metadata_feature_columns = fixed_metadata_features + word_freq_features

    # Create feature matrix and target vector
X_metadata = metadata_df[metadata_feature_columns]
y = metadata_df['label']

# 1.4 Consistent Train/Val/Test Split
# Using the exact same parameters as your BERT split (64/16/20)
print("Splitting data into Train (64%), Validation (16%), Test (20%)...")
TEST_RATIO = 0.20
VAL_RATIO_FROM_REMAINING = 0.20 # 0.16 / (1.0 - 0.20) = 0.16 / 0.80 = 0.20
RANDOM_STATE = 42 # Use the same random_state as your BERT split

# Split off Test set first
X_meta_trainval, X_meta_test, y_trainval, y_test = train_test_split(
    X_metadata, y,
    test_size=TEST_RATIO,
    random_state=RANDOM_STATE,
    stratify=y # Use stratify for balanced labels if possible
)
# Split remaining TrainVal into Train and Validation
X_meta_train, X_meta_val, y_train, y_val = train_test_split(
    X_meta_trainval, y_trainval,
    test_size=VAL_RATIO_FROM_REMAINING,
    random_state=RANDOM_STATE,
    stratify=y_trainval # Use stratify here too
)

print(f"Train shapes: X={X_meta_train.shape}, y={y_train.shape}")
print(f"Validation shapes: X={X_meta_val.shape}, y={y_val.shape}")
print(f"Test shapes: X={X_meta_test.shape}, y={y_test.shape}")

# 1.5 Scale Features (Fit ONLY on Train data)
print("Scaling metadata features...")
scaler = StandardScaler()
X_meta_train_scaled = scaler.fit_transform(X_meta_train) # Fit only on training data
X_meta_val_scaled = scaler.transform(X_meta_val)       # Transform validation data
X_meta_test_scaled = scaler.transform(X_meta_test)      # Transform test data

# Save the fitted scaler
#joblib.dump(scaler, SCALER_PATH)
#print(f"Scaler fitted on training data and saved to {SCALER_PATH}")

# 1.6 Train the Metadata Model
print("\nTraining metadata model (e.g., RandomForest) on scaled training data...")
# Using RandomForestClassifier as specified in your proposal/previous code attempt
# class_weight='balanced' helps if your labels (0 vs 1) are not perfectly 50/50
metadata_model = RandomForestClassifier(
    n_estimators=100,       # Number of trees (default=100, can tune)
    random_state=RANDOM_STATE,
    n_jobs=-1,              # Use all available CPU cores
    class_weight='balanced'
)
#metadata_model.fit(X_meta_train_scaled, y_train)
#print("Metadata model training complete.")

# 1.7 Save the Trained Model
#joblib.dump(metadata_model, METADATA_MODEL_PATH)
#print(f"Trained metadata model saved to {METADATA_MODEL_PATH}")

'''
RandomForest Metadata + Text Model Accuracy: 0.9572556264696003

              precision    recall  f1-score   support

         0.0       0.97      0.95      0.96     11897
         1.0       0.95      0.97      0.96     11919

    accuracy                           0.96     23816
   macro avg       0.96      0.96      0.96     23816
weighted avg       0.96      0.96      0.96     23816
'''

from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
# 1.8 Evaluate Model Performance on Validation Set
# This tells you how well the metadata *alone* performs on data unseen during training
print("\nEvaluating metadata model performance on VALIDATION set...")
y_pred_val = metadata_model.predict(X_meta_val_scaled)
val_accuracy = accuracy_score(y_val, y_pred_val)
val_f1 = f1_score(y_val, y_pred_val) # F1 for positive class (Genuine=1)

print(f"Validation Accuracy: {val_accuracy:.4f}")
print(f"Validation F1-Score (Class 1): {val_f1:.4f}")
print("Validation Classification Report:\n", classification_report(y_val, y_pred_val, digits=4))
print("\nValidation Confusion Matrix:\n", confusion_matrix(y_val, y_pred_val))

print("\n--- Metadata Model Development Complete ---")

# --- Corrected Test Evaluation Block ---

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

print("\nEvaluating metadata model performance on TEST set (using SCALED data)...")

# --- FIX IS HERE: Use X_meta_test_scaled ---
y_test_pred = metadata_model.predict(X_meta_test_scaled)
# --- END FIX ---

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred) # F1 for positive class (Genuine=1)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test F1-Score (Class 1): {test_f1:.4f}")
print("Test Classification Report:\n", classification_report(y_test, y_test_pred, digits=4))
print("Test Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))

#!pip uninstall textstat -y  # Uninstall if previously installed

#!rm -rf /usr/local/lib/python3.11/dist-packages/textstat

#pip install --no-cache-dir textstat==0.7.3

import textstat

# 1. Setup (Imports)
# Ensure necessary libraries are installed
#!pip install shap # Or a recent version compatible with your environment
#!pip install transformers torch scikit-learn pandas joblib textstat nltk matplotlib contractions
#!pip install contractions

import shap
import torch
import transformers
from transformers import BertForSequenceClassification, BertTokenizer
import joblib
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords, wordnet # Added wordnet for POS tagger
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import string
import re
import contractions
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from sklearn.model_selection import train_test_split # Needed to get X_train for vectorizer fitting
from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.preprocessing import StandardScaler # Should be loaded via scaler object, but import for clarity

print(f"SHAP version installed: {shap.__version__}")
print(f"Transformers version installed: {transformers.__version__}")

# 4. Setup for Metadata Extraction (Fit Vectorizer correctly)
#from google.colab import drive
#drive.mount('/content/gdrive')
# Define paths
'''
GDRIVE_PATH = '/content/gdrive/My Drive/'
METADATA_CSV_PATH = os.path.join(GDRIVE_PATH, 'reviews_with_metadata.csv')
BERT_MODEL_DIR = os.path.join(GDRIVE_PATH, 'best_bert_model_50epochs')
SCALER_PATH = os.path.join(GDRIVE_PATH, 'metadata_scaler.joblib')
METADATA_MODEL_PATH = os.path.join(GDRIVE_PATH, 'metadata_model.joblib')
df = pd.read_csv('/content/gdrive/My Drive/reviews_with_metadata.csv') # Load the DataFrame
'''

# --- 2. Download NLTK Resources (Simplified) ---
print("Checking/Downloading NLTK resources...")
nltk_resources_to_download = {
    'punkt': 'tokenizers/punkt',
    'stopwords': 'corpora/stopwords',
    'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',
    'wordnet': 'corpora/wordnet',
    'vader_lexicon': 'sentiment/vader_lexicon.zip' # NLTK download handles .zip correctly
}
for resource_name, resource_path in nltk_resources_to_download.items():
    try:
        nltk.data.find(resource_path)
        print(f"NLTK resource '{resource_name}' found.")
    except LookupError:
        print(f"Downloading NLTK resource '{resource_name}'...")
        nltk.download(resource_name, quiet=True)
print("NLTK resources ready.")

# 3. Define Helper Functions (Consistent Preprocessing)

stop_words_set = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
analyzer = SentimentIntensityAnalyzer() # Initialize VADER once

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN) # Default to noun

def clean_text(text):
    """Applies comprehensive text cleaning. Used for BERT and as a step for metadata"""
    if not isinstance(text, str): text = str(text) # Ensure string

    text = contractions.fix(text)  # Expand contractions
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters, keep spaces
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces

    words = word_tokenize(text)
    cleaned_words = []
    for word in words:
        if word not in stop_words_set and len(word) > 1:
            lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))
            cleaned_words.append(lemma)
    return ' '.join(cleaned_words)

def get_sentiment(text):
     """Calculates VADER compound sentiment score."""
     # analyzer should be initialized globally
     return analyzer.polarity_scores(text)['compound'] if isinstance(text, str) else 0.0

def get_flesch_reading_ease(text):
     """Calculates Flesch reading ease, handles errors."""
     if isinstance(text, str) and len(text.split()) > 4: # Basic check
         try:
             # Textstat might fail on very short/unusual texts
             return textstat.flesch_reading_ease(text)
         except Exception as e:
             # print(f"Textstat warning for text '{text[:50]}...': {e}")
             return 0.0 # Return default on error
     return 0.0 # Default for short/invalid text

def count_stopwords_in_raw_text(text): # Renamed to be specific
    """Counts stopwords in raw (uncleaned) text."""
    if isinstance(text, str):
        words = text.lower().split() # Simple split is fine here
        return sum(1 for word in words if word in stop_words_set)
    return 0

import nltk
nltk.download('punkt_tab')
import nltk
nltk.download('averaged_perceptron_tagger_eng')
# --- Fit CountVectorizer ONLY on Training Data Text ---
print("Loading data to fit CountVectorizer on training split...")
vectorizer = None # Initialize
feature_names_list_vectorizer = []
try:
    temp_df_for_split = pd.read_csv(METADATA_CSV_PATH, usecols=['text_', 'label'])
    temp_df_for_split['text_'] = temp_df_for_split['text_'].fillna("")
    temp_df_for_split.dropna(subset=['label'], inplace=True)
    temp_df_for_split['label'] = temp_df_for_split['label'].astype(int)

    X_text_full = temp_df_for_split['text_']
    y_full = temp_df_for_split['label']

    # Replicate the first split to get the 80% train+val portion
    RANDOM_STATE = 42
    TEST_RATIO = 0.20
    X_text_trainval, _, y_text_trainval, _ = train_test_split(
        X_text_full, y_full, test_size=TEST_RATIO, random_state=RANDOM_STATE, stratify=y_full
    )
    # Replicate the second split to get the 64% train portion
    VAL_RATIO_FROM_REMAINING = 0.20
    X_text_train, _, y_text_train, _ = train_test_split(
        X_text_trainval, y_text_trainval, test_size=VAL_RATIO_FROM_REMAINING, random_state=RANDOM_STATE, stratify=y_text_trainval
    )

    print(f"Fitting CountVectorizer on {len(X_text_train)} training samples...")
    # Apply final cleaning before fitting vectorizer
    X_text_train_cleaned = X_text_train.apply(clean_text)

    vectorizer = CountVectorizer(max_features=100, stop_words='english') # Use 100 features as defined before
    vectorizer.fit(X_text_train_cleaned) # Fit ONLY on cleaned training text
    feature_names_list_vectorizer = vectorizer.get_feature_names_out().tolist()
    print(f"Vectorizer fitted. Vocabulary size: {len(feature_names_list_vectorizer)}")
    del temp_df_for_split, X_text_full, y_full, X_text_trainval, y_text_trainval, X_text_train, y_text_train, X_text_train_cleaned # Clean up memory

except FileNotFoundError:
    print(f"ERROR: Could not find CSV at {METADATA_CSV_PATH} to fit vectorizer.")
    vectorizer = None
    feature_names_list_vectorizer = []
except Exception as e:
    print(f"ERROR during vectorizer fitting: {e}")
    vectorizer = None
    feature_names_list_vectorizer = []


# --- Define the FULL list of metadata features the model expects ---
# Make sure this matches EXACTLY what metadata_model.joblib was trained on
base_metadata_features = [
    'sentiment', 'review_length', 'uppercase_word_count', 'exclamation_count',
    'question_count', 'punctuation_count', 'avg_word_length',
    'flesch_reading_ease', 'stopword_count'
]
# Combine base features with the word frequency feature names from the vectorizer
metadata_feature_columns = base_metadata_features + feature_names_list_vectorizer
print(f"Total metadata features expected by model: {len(metadata_feature_columns)}")

# 5. Corrected Metadata Extraction Function

def extract_metadata_for_text(text, feature_names_list, vectorizer_obj, feature_names_list_vectorizer):
    """Extracts all metadata including word frequencies for a single text."""
    features = {}
    text_raw = str(text) if pd.notna(text) else ""
    cleaned_text = clean_text(text_raw)

    # --- Calculate Base Features ---
    features['sentiment'] = get_sentiment(cleaned_text)
    features['review_length'] = len(cleaned_text.split()) if cleaned_text else 0
    features['uppercase_word_count'] = sum(1 for word in text_raw.split() if word.isupper())
    features['exclamation_count'] = text_raw.count('!')
    features['question_count'] = text_raw.count('?')
    features['punctuation_count'] = sum(1 for char in text_raw if char in string.punctuation)
    split_words_cleaned = cleaned_text.split()
    features['avg_word_length'] = np.mean([len(w) for w in split_words_cleaned]) if split_words_cleaned else 0
    features['flesch_reading_ease'] = get_flesch_reading_ease(text_raw)
    features['stopword_count'] = count_stopwords_in_raw_text(cleaned_text)

    # --- Word Frequency ---
    if vectorizer_obj and feature_names_list_vectorizer:
        try:
            word_count_array = vectorizer_obj.transform([cleaned_text]).toarray()[0]
            if len(word_count_array) == len(feature_names_list_vectorizer):
                for i, word in enumerate(feature_names_list_vectorizer):
                    features[word] = word_count_array[i]
            else:
                print(f"Warning: Vectorizer output length mismatch: {len(word_count_array)} vs {len(feature_names_list_vectorizer)}")
                for word in feature_names_list_vectorizer:
                    features[word] = 0
        except Exception as e:
            print(f"Warning: CountVectorizer failed for text: {e}")
            for word in feature_names_list_vectorizer:
                features[word] = 0
    elif feature_names_list_vectorizer:
        for word in feature_names_list_vectorizer:
            features[word] = 0

    # --- Final Check & Order ---
    final_ordered_features = {}
    for col in feature_names_list:
        final_ordered_features[col] = features.get(col, 0)

    # Debugging: Print the number of features extracted
    print(f"Number of features extracted for text: {len(final_ordered_features)}")
    return final_ordered_features

    # Sample test input
sample_text = "This product is AMAZING!!! Loved it! Would buy again?"

# Call the function and print result
result = extract_metadata_for_text(sample_text, metadata_feature_columns, vectorizer, feature_names_list_vectorizer)
print("Extracted Metadata:")
print(result)

# 6. Load Models & Assets
print("\nLoading models and assets for XAI...")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model = BertForSequenceClassification.from_pretrained(BERT_MODEL_DIR).to(device).eval()
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)
metadata_model = joblib.load(METADATA_MODEL_PATH)
scaler = joblib.load(SCALER_PATH)
print("Models, tokenizer, and scaler loaded.")

# 7. Define Prediction Wrappers for SHAP (Corrected Metadata Wrapper)
print("Defining prediction wrappers for SHAP...")
def bert_predict_proba_shap(text_list):
    """Wrapper for BERT prediction returning probabilities."""
    bert_model.eval()
    # (Code is the same as previous correct version - handles lists, tokenizes, predicts, returns numpy probs)
    try:
        if isinstance(text_list, str): text_list = [text_list]
        if not isinstance(text_list, list): text_list = list(text_list)
        inputs = tokenizer(text_list, return_tensors='pt', truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = bert_model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        return probabilities
    except Exception as e:
        print(f"Error in BERT prediction wrapper: {e}")
        return np.array([[0.5, 0.5]] * len(text_list))

def metadata_predict_proba_shap(text_list):
    """Wrapper for Metadata model: text list -> extract -> scale -> predict proba."""
    #metadata_model.eval() # Not needed for sklearn RF, but good practice
    try:
        if isinstance(text_list, str): text_list = [text_list]
        if not isinstance(text_list, list): text_list = list(text_list)

        processed_data_list = []
        # Access the globally fitted vectorizer and its feature names
        global vectorizer, feature_names_list_vectorizer

        for text in text_list:
            # Call the corrected extraction function
            features_dict = extract_metadata_for_text(text, metadata_feature_columns, vectorizer, feature_names_list_vectorizer)


            # Convert dict to ordered list based on metadata_feature_columns
            feature_values_ordered = [features_dict.get(col, 0) for col in metadata_feature_columns]
            processed_data_list.append(feature_values_ordered)

        if not processed_data_list:
             return np.zeros((0, metadata_model.n_classes_))

        meta_features_array = np.array(processed_data_list)

        # Check for NaNs before scaling
        if np.isnan(meta_features_array).any():
             print("Warning: NaNs detected in metadata features before scaling. Filling with 0.")
             meta_features_array = np.nan_to_num(meta_features_array, nan=0.0)

        # Scale features
        scaled_features = scaler.transform(meta_features_array)

        # Predict probabilities
        probabilities = metadata_model.predict_proba(scaled_features)
        return probabilities
    except Exception as e:
        print(f"Error in Metadata prediction wrapper: {e}")
        return np.array([[0.5, 0.5]] * len(text_list))

# 8. Initialize SHAP Explainers
print("Initializing SHAP explainers...")
# BERT Explainer
masker_bert = shap.maskers.Text(tokenizer)
explainer_bert = shap.Explainer(bert_predict_proba_shap, masker_bert, output_names=['Fake (OR)', 'Genuine (CG)'])
print("BERT Explainer initialized.")

# Metadata Explainer (TreeExplainer for RandomForest)
# Pass the expected feature names from training
# Metadata Explainer (TreeExplainer for RandomForest)

# Pass the expected feature names from training

explainer_meta = shap.TreeExplainer(metadata_model, feature_names=metadata_feature_columns, feature_perturbation='interventional')

print("Metadata Explainer initialized.")

import matplotlib.pyplot as plt
import numpy as np
import nltk
shap.initjs()
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
print("\nGenerating explanations for sample text...")

sample_reviews = [
    "This is fantastic! It works exactly as described and has made my life so much easier. Quality is top-notch. Would definitely buy again and recommend to all my friends.",
    "Total garbage. Fell apart after one use. Customer service was useless. Avoid this product and this company like the plague. Complete waste of money want refund now!!!",
    "Click now amazing prize just for you exclusive offer limited time opportunity win big today special discount buy here cheap price best ever."
]
class_names = ['Fake (OR)', 'Genuine (CG)']

for review_text in sample_reviews:
    print(f"\n--- Explaining Review: '{review_text[:100]}...' ---")

    try:
       # BERT prediction and SHAP
       bert_probs = bert_predict_proba_shap(review_text)[0];
       bert_pred_label = np.argmax(bert_probs)

       print(f"  BERT Prediction: {class_names[bert_pred_label]} (Probs: Fake={bert_probs[0]:.2f}, Genuine={bert_probs[1]:.2f})")

    # --- BERT Explanation ---
       print("\nBERT Explanation:")
       shap_values_bert = explainer_bert([review_text])
       shap.plots.text(shap_values_bert[0]) # Explains all classes for the instance
       plt.show()

       print(f"\n  Displaying SHAP waterfall plot for BERT (Predicted Class: {class_names[bert_pred_label]})...")
       shap.plots.waterfall(shap_values_bert[0, :, bert_pred_label], max_display=12)
       plt.show()

    # --- Metadata Explanation ---
       print("\nMetadata Explanation:")
       features_dict_instance = extract_metadata_for_text(
           review_text, metadata_feature_columns, vectorizer, feature_names_list_vectorizer
       )
       feature_values_ordered = np.array([[features_dict_instance.get(col, 0) for col in metadata_feature_columns]])
       feature_values_ordered = np.nan_to_num(feature_values_ordered, nan=0.0)
       scaled_features_instance = scaler.transform(feature_values_ordered)

       shap_values_meta = explainer_meta.shap_values(scaled_features_instance, check_additivity=False)


       # Determine if shap_values_meta is a list (multi-class) or single array (binary/single output)
       if isinstance(shap_values_meta, list) and len(shap_values_meta) > 1:
           # Multi-class case
           shap_values_instance = shap_values_meta[bert_pred_label][0]
           base_value = explainer_meta.expected_value[bert_pred_label]
       elif isinstance(shap_values_meta, list):
            # Single-class case (binary classification)
           shap_values_instance = shap_values_meta[0][0]
           base_value = explainer_meta.expected_value[0] if isinstance(explainer_meta.expected_value, (list, np.ndarray)) else explainer_meta.expected_value
       else:
           shap_values_instance = shap_values_meta[0]
           base_value = explainer_meta.expected_value
       shap_values_instance = shap_values_meta[0][:, bert_pred_label]  # shape (109,)
       base_value = base_value[bert_pred_label]

       explanation_meta = shap.Explanation(
           values=np.array(shap_values_instance),
           base_values=float(base_value),
           data=scaled_features_instance[0],
           feature_names=metadata_feature_columns
       )

       print(f"  Displaying SHAP bar plot for Metadata (Explaining BERT's predicted class: {class_names[bert_pred_label]})...")
       shap.plots.bar(explanation_meta, max_display=15)
       plt.tight_layout()
       plt.show()

       print(f"\n  Displaying SHAP force plot for Metadata (Class: {class_names[bert_pred_label]})...")
       shap.force_plot(
           base_value,
           shap_values_instance,
           features=scaled_features_instance[0],
           feature_names=metadata_feature_columns,
           matplotlib=True,
           show=True
       )
       plt.show()

    except Exception as e:
        print(f"  Could not generate Metadata SHAP plot: {e}")

import joblib

# Load SHAP explainers (saved earlier)
#explainer_bert = joblib.load('models/bert_explainer.pkl')
#explainer_meta = joblib.load('models/meta_explainer.pkl')

print(bert_predict_proba_shap(["This is great!"]))
print(metadata_predict_proba_shap(["This is great!"]))

# 1. Imports
import torch
import transformers
from transformers import BertForSequenceClassification, BertTokenizer
import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from torch.utils.data import DataLoader, TensorDataset, SequentialSampler
import os
import time

# --- Define Paths & Feature Names ---
#from google.colab import drive
#drive.mount('/content/gdrive')
# Define paths
'''
GDRIVE_PATH = '/content/gdrive/My Drive/'
METADATA_CSV_PATH = os.path.join(GDRIVE_PATH, 'reviews_with_metadata.csv')
BERT_MODEL_DIR = os.path.join(GDRIVE_PATH, 'best_bert_model_50epochs')
SCALER_PATH = os.path.join(GDRIVE_PATH, 'metadata_scaler.joblib')
METADATA_MODEL_PATH = os.path.join(GDRIVE_PATH, 'metadata_model.joblib')
df = pd.read_csv('/content/gdrive/My Drive/reviews_with_metadata.csv') # Load the DataFrame
'''

from sklearn.feature_extraction.text import CountVectorizer
# --- Define the EXACT list of metadata features ---
# --- CRITICAL: Must match the features used to train metadata_model.joblib ---
base_metadata_features = [
    'sentiment', 'review_length', 'uppercase_word_count', 'exclamation_count',
    'question_count', 'punctuation_count', 'avg_word_length',
    'flesch_reading_ease', 'stopword_count'
]
# Define vectorizer and extract top 100 word frequencies from the text column
vectorizer = CountVectorizer(max_features=100, stop_words='english')
df['text_'] = df['text_'].fillna('')  # Replace NaN with empty string
word_counts = vectorizer.fit_transform(df['text_']).toarray()

# Get the feature names from the vectorizer (i.e., words like "great", "bad", etc.)
feature_names_list_vectorizer = vectorizer.get_feature_names_out().tolist()

# Combine basic metadata features with word frequency features
metadata_feature_columns = base_metadata_features + feature_names_list_vectorizer

# 2. Load Models & Assets
print("Loading models and assets for final evaluation...")
try:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    bert_model = BertForSequenceClassification.from_pretrained(BERT_MODEL_DIR).to(device).eval()
    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)
    metadata_model = joblib.load(METADATA_MODEL_PATH)
    scaler = joblib.load(SCALER_PATH)
    # --- Verify/Load feature names from model if possible ---
    if hasattr(metadata_model, 'feature_names_in_'):
        metadata_feature_columns = metadata_model.feature_names_in_.tolist()
        print(f"Loaded feature names from metadata model: {len(metadata_feature_columns)} features.")
    else:
        print(f"Warning: Could not load feature names from model. Using manually defined list ({len(metadata_feature_columns)} features).")
        if len(metadata_feature_columns) <= len(base_metadata_features):
             print("ERROR: Word frequency feature names seem missing!")

    print("Models, tokenizer, and scaler loaded successfully.")
except Exception as e:
    print(f"FATAL ERROR: Could not load models/assets: {e}")
    exit()

# 3. Load and Prepare TEST Data
print("Loading and preparing test data...")
try:
    metadata_df = pd.read_csv(METADATA_CSV_PATH)
    # Handle NaNs same way as in prediction_engine
    metadata_df.dropna(subset=['label'], inplace=True)
    metadata_df['label'] = metadata_df['label'].astype(int)
    numeric_cols = metadata_df.select_dtypes(include=np.number).columns.tolist()
    if 'label' in numeric_cols: numeric_cols.remove('label')
    metadata_df[numeric_cols] = metadata_df[numeric_cols].fillna(metadata_df[numeric_cols].median())

    X = metadata_df['text_'].fillna("") # Ensure text column is clean
    X_meta = metadata_df[metadata_feature_columns]
    y = metadata_df['label']

    # Replicate the test split exactly
    TEST_RATIO = 0.20; RANDOM_STATE = 42
    X_temp_text, X_test_text, y_temp_text, y_test = train_test_split(X, y, test_size=TEST_RATIO, random_state=RANDOM_STATE, stratify=y)
    # Get corresponding metadata test set using the index
    X_meta_test = X_meta.loc[X_test_text.index]
    if not np.array_equal(y_test.values, y.loc[X_test_text.index].values):
         print("CRITICAL WARNING: Label mismatch in test set regeneration!")

    # Scale metadata test features
    X_meta_test_scaled = scaler.transform(X_meta_test)
    print(f"Test set prepared. Text samples: {len(X_test_text)}, Metadata samples: {X_meta_test_scaled.shape[0]}, Labels: {len(y_test)}")
except Exception as e:
    print(f"ERROR loading or preparing test data: {e}")
    exit()

# 4. Get Predictions from Both Models on Test Set
start_time = time.time()
# --- BERT Predictions ---
print("Getting BERT predictions on test set...")
try:
    test_encodings = tokenizer(list(X_test_text), truncation=True, padding=True, max_length=512)
    test_dataset_bert = TensorDataset(torch.tensor(test_encodings['input_ids']),
                                      torch.tensor(test_encodings['attention_mask']))
    eval_batch_size = 32 # Can increase for inference
    test_dataloader_bert = DataLoader(test_dataset_bert, sampler=SequentialSampler(test_dataset_bert), batch_size=eval_batch_size)

    bert_test_probs_list = []
    with torch.no_grad():
        for batch in test_dataloader_bert:
            input_ids = batch[0].to(device)
            attention_mask = batch[1].to(device)
            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)
            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
            bert_test_probs_list.extend(probs)
    bert_test_probs = np.array(bert_test_probs_list) # Shape: (n_samples, 2)
    print(f"BERT predictions obtained. Shape: {bert_test_probs.shape}")
except Exception as e:
    print(f"ERROR during BERT test prediction: {e}")
    exit()

# --- Metadata Predictions ---
print("Getting Metadata model predictions on test set...")
try:
    meta_test_probs = metadata_model.predict_proba(X_meta_test_scaled) # Shape: (n_samples, 2)
    print(f"Metadata predictions obtained. Shape: {meta_test_probs.shape}")
except Exception as e:
    print(f"ERROR during Metadata test prediction: {e}")
    exit()

# 5. Apply Ensemble Logic (Simple Averaging)
print("Applying ensemble logic...")
# Probabilities for the 'Genuine' class (assuming index 1)
bert_prob_genuine_test = bert_test_probs[:, 1]
meta_prob_genuine_test = meta_test_probs[:, 1]

# Average the probabilities
final_prob_genuine_test = (bert_prob_genuine_test + meta_prob_genuine_test) / 2.0

# Classify based on threshold 0.5
y_pred_ensemble = (final_prob_genuine_test > 0.5).astype(int)
print("Ensemble predictions calculated.")

# 6. Calculate Final Ensemble Metrics
print("\n=== Final ENSEMBLE Evaluation on Test Set ===")
accuracy_ens = accuracy_score(y_test, y_pred_ensemble)
precision_ens = precision_score(y_test, y_pred_ensemble)
recall_ens = recall_score(y_test, y_pred_ensemble)
f1_ens = f1_score(y_test, y_pred_ensemble)

print(f"Ensemble Test Accuracy: {accuracy_ens:.4f}")
print(f"Ensemble Test Precision: {precision_ens:.4f}")
print(f"Ensemble Test Recall: {recall_ens:.4f}")
print(f"Ensemble Test F1-Score: {f1_ens:.4f}")
print("\nEnsemble Classification Report:")
print(classification_report(y_test, y_pred_ensemble, digits=4))
print("\nEnsemble Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ensemble))

# Compare with BERT-only results (load/paste your previous BERT test results)
print("\n--- Comparison with BERT-Only Test Results ---")
print("---------------------------------------------")


end_time = time.time()
print(f"\nFinal evaluation script took {end_time - start_time:.2f} seconds.")
print("\n--- Final Ensemble Evaluation Complete ---")

